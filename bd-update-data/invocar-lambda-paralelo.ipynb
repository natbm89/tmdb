{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f801aa",
   "metadata": {},
   "source": [
    "# Invocación Paralela de Lambda para Procesamiento de Películas\n",
    " \n",
    "Este notebook automatiza el procesamiento masivo de archivos JSON almacenados en S3 mediante la invocación paralela de funciones AWS Lambda. El objetivo es cargar grandes volúmenes de datos de películas en una base de datos de forma eficiente, controlada y monitorizada.\n",
    "\n",
    "**Flujo principal:**\n",
    "- Obtiene la lista de archivos JSON a procesar desde un bucket S3, excluyendo archivos irrelevantes (como `genres.json`).\n",
    "- Inicializa un objeto de estadísticas thread-safe para monitorizar el progreso y los resultados.\n",
    "- Define una función robusta para invocar la Lambda con reintentos automáticos y registro de métricas.\n",
    "- Lanza el procesamiento paralelo de todos los archivos usando un pool de hasta 3 Lambdas concurrentes, con reintentos automáticos en caso de error.\n",
    "- Muestra el progreso en tiempo real y reporta estadísticas finales de éxito, fallo y rendimiento.\n",
    "\n",
    "**Configuración Recomendada para AWS Lambda:**\n",
    "- **Timeout:** 900 segundos (15 minutos) - máximo permitido\n",
    "- **Memoria:** 1024 MB (1 GB) - balance óptimo rendimiento/costo\n",
    "- **Runtime:** Python 3.11 o superior\n",
    "- **Arquitectura:** x86_64\n",
    "\n",
    "**Variables necesarias:**\n",
    "- Credenciales de base de datos (desde Secrets Manager)\n",
    "- Configuración de región AWS y nombre del secreto en variables del entorno\n",
    "\n",
    "El reprocesamiento de archivos fallidos se realiza en una fase posterior, reutilizando la misma lógica de procesamiento paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f00de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "import time\n",
    "import threading\n",
    "from threading import Lock\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de AWS\n",
    "bucket_name = os.getenv('BUCKET_NAME_E')  # Data lake original\n",
    "lambda_function_name = 'volcado-completo'\n",
    "region_name = os.getenv('REGION')\n",
    "\n",
    "# Configuración de procesamiento paralelo\n",
    "MAX_CONCURRENT_LAMBDAS = 3\n",
    "RETRY_ATTEMPTS = 3  # Reintentos automáticos\n",
    "LAMBDA_TIMEOUT = 900  # 15 minutos timeout para Lambda (máximo permitido)\n",
    "\n",
    "# Inicializar clientes AWS\n",
    "s3 = boto3.client('s3')\n",
    "lambda_client = boto3.client('lambda', region_name=region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para estadísticas thread-safe\n",
    "class ProcessingStats:\n",
    "    def __init__(self):\n",
    "        self.lock = Lock()\n",
    "        self.total_files = 0\n",
    "        self.completed_files = 0\n",
    "        self.failed_files = []\n",
    "        self.successful_files = []\n",
    "        self.total_movies_processed = 0\n",
    "        self.total_movies_inserted = 0\n",
    "        self.total_movies_updated = 0\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "    def add_success(self, filename, processed, inserted, updated):\n",
    "        with self.lock:\n",
    "            self.completed_files += 1\n",
    "            self.successful_files.append(filename)\n",
    "            self.total_movies_processed += processed\n",
    "            self.total_movies_inserted += inserted\n",
    "            self.total_movies_updated += updated\n",
    "    \n",
    "    def add_failure(self, filename, error):\n",
    "        with self.lock:\n",
    "            self.completed_files += 1\n",
    "            self.failed_files.append({'file': filename, 'error': str(error)})\n",
    "    \n",
    "    def get_progress(self):\n",
    "        with self.lock:\n",
    "            elapsed_time = (datetime.now() - self.start_time).total_seconds()\n",
    "            completion_rate = self.completed_files / self.total_files if self.total_files > 0 else 0\n",
    "            eta_seconds = (elapsed_time / completion_rate * (1 - completion_rate)) if completion_rate > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'completed': self.completed_files,\n",
    "                'total': self.total_files,\n",
    "                'successful': len(self.successful_files),\n",
    "                'failed': len(self.failed_files),\n",
    "                'movies_processed': self.total_movies_processed,\n",
    "                'movies_inserted': self.total_movies_inserted,\n",
    "                'movies_updated': self.total_movies_updated,\n",
    "                'elapsed_minutes': elapsed_time / 60,\n",
    "                'eta_minutes': eta_seconds / 60,\n",
    "                'completion_percentage': completion_rate * 100\n",
    "            }\n",
    "\n",
    "# Inicializar estadísticas\n",
    "stats = ProcessingStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a825ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_lambda_for_file(file_key, attempt=1, retry_attempts=3, delay_base=1, stats_obj=None, show_skipped=False):\n",
    "    # Invoca Lambda para procesar un archivo específico con reintentos automáticos\n",
    "    if stats_obj is None:\n",
    "        stats_obj = stats\n",
    "    try:\n",
    "        # Preparar payload para Lambda\n",
    "        payload = {\n",
    "            'bucket_name': bucket_name,\n",
    "            'file_key': file_key\n",
    "        }\n",
    "        \n",
    "        print(f\"[{attempt}/{retry_attempts}] Invocando Lambda para: {file_key}\")\n",
    "        \n",
    "        # Invocar Lambda de forma síncrona\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=lambda_function_name,\n",
    "            InvocationType='RequestResponse',\n",
    "            Payload=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        # Leer respuesta\n",
    "        response_payload = json.loads(response['Payload'].read().decode('utf-8'))\n",
    "        \n",
    "        if response['StatusCode'] == 200 and response_payload.get('statusCode') == 200:\n",
    "            # Procesar respuesta exitosa\n",
    "            body = json.loads(response_payload['body'])\n",
    "            processed = body.get('processed', 0)\n",
    "            inserted = body.get('inserted', 0)\n",
    "            updated = body.get('updated', 0)\n",
    "            skipped = body.get('skipped', 0) if show_skipped else 0\n",
    "            \n",
    "            stats_obj.add_success(file_key, processed, inserted, updated)\n",
    "            print(f\"ÉXITO {file_key}: {processed} películas ({inserted} nuevas, {updated} actualizadas)\")\n",
    "            if show_skipped and skipped > 0:\n",
    "                print(f\"   Saltadas {skipped} películas sin título válido\")\n",
    "            return True\n",
    "        else:\n",
    "            # Error en la respuesta\n",
    "            error_msg = response_payload.get('body', 'Error desconocido')\n",
    "            raise Exception(f\"Lambda error: {error_msg}\")\n",
    "    except Exception as e:\n",
    "        if attempt < retry_attempts:\n",
    "            delay = delay_base * (2 ** (attempt - 1))\n",
    "            print(f\"Error en intento {attempt}, esperando {delay}s... {str(e)[:100]}\")\n",
    "            time.sleep(delay)\n",
    "            return invoke_lambda_for_file(file_key, attempt + 1, retry_attempts, delay_base, stats_obj, show_skipped)\n",
    "        else:\n",
    "            print(f\"ERROR DEFINITIVO en {file_key}: {str(e)}\")\n",
    "            stats_obj.add_failure(file_key, e)\n",
    "            return False\n",
    "\n",
    "def process_file_wrapper(file_key):\n",
    "    # Wrapper para el procesamiento en ThreadPoolExecutor\n",
    "    return invoke_lambda_for_file(file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_processing(file_list, stats_obj, max_workers=MAX_CONCURRENT_LAMBDAS, retry_attempts=3, delay_base=1, show_skipped=False):\n",
    "    # Procesa en paralelo una lista de archivos usando invoke_lambda_for_file y ThreadPoolExecutor.\n",
    "    stop_event = threading.Event()\n",
    "    monitor_thread = threading.Thread(target=monitor_progress_generic, args=(stop_event, stats_obj))\n",
    "    monitor_thread.daemon = True\n",
    "    monitor_thread.start()\n",
    "\n",
    "    stats_obj.start_time = datetime.now()\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_file = {\n",
    "                executor.submit(invoke_lambda_for_file, file_key, 1, retry_attempts, delay_base, stats_obj, show_skipped): file_key\n",
    "                for file_key in file_list\n",
    "            }\n",
    "            for future in concurrent.futures.as_completed(future_to_file):\n",
    "                file_key = future_to_file[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inesperado en {file_key}: {str(e)}\")\n",
    "                    stats_obj.add_failure(file_key, e)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcesamiento interrumpido por el usuario\")\n",
    "        stop_event.set()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError crítico en el procesamiento: {str(e)}\")\n",
    "        stop_event.set()\n",
    "        raise e\n",
    "\n",
    "    finally:\n",
    "        stop_event.set()\n",
    "        if monitor_thread.is_alive():\n",
    "            monitor_thread.join(timeout=2)\n",
    "        end_time = datetime.now()\n",
    "        total_duration = (end_time - start_time).total_seconds()\n",
    "        return total_duration\n",
    "\n",
    "def monitor_progress_generic(stop_event, stats_obj):\n",
    "    # Funcion para monitoreo\n",
    "    while not stop_event.is_set():\n",
    "        progress = stats_obj.get_progress()\n",
    "        if progress['total'] > 0:\n",
    "            print(f\"\\nPROGRESO:\")\n",
    "            print(f\" Archivos: {progress['completed']}/{progress['total']} ({progress['completion_percentage']:.1f}%)\")\n",
    "            print(f\" Exitosos: {progress['successful']} | Fallidos: {progress['failed']}\")\n",
    "            print(f\" Películas: {progress['movies_processed']} ({progress['movies_inserted']} nuevas, {progress['movies_updated']} actualizadas)\")\n",
    "            print(f\" Tiempo: {progress['elapsed_minutes']:.1f} min | ETA: {progress['eta_minutes']:.1f} min\")\n",
    "            print(\"-\" * 50)\n",
    "        stop_event.wait(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener lista de archivos del bucket\n",
    "print(\"Listando archivos del bucket S3...\")\n",
    "\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "    all_keys = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "    \n",
    "    # Extensiones y archivos específicos a ignorar\n",
    "    excluidos_ext = ['.txt']\n",
    "    excluidos_files = ['genres.json']\n",
    "    \n",
    "    # Filtrar archivos: excluir extensiones no deseadas Y archivos específicos\n",
    "    keys = [key for key in all_keys \n",
    "            if not any(key.endswith(ext) for ext in excluidos_ext)\n",
    "            and not any(key.lower().endswith(filename.lower()) for filename in excluidos_files)]\n",
    "    \n",
    "    # Configurar estadísticas\n",
    "    stats.total_files = len(keys)\n",
    "    \n",
    "    print(f\"RESUMEN DE ARCHIVOS:\")\n",
    "    print(f\"Total archivos en bucket: {len(all_keys)}\")\n",
    "    print(f\"Archivos JSON a procesar: {len(keys)}\")\n",
    "    print(f\"Archivos excluidos: {len(all_keys) - len(keys)}\")\n",
    "    \n",
    "    # Mostrar archivos excluidos si los hay\n",
    "    excluidos_encontrados = [key for key in all_keys if key not in keys]\n",
    "    if excluidos_encontrados:\n",
    "        print(f\"Archivos excluidos: {excluidos_encontrados}\")\n",
    "    \n",
    "    print(f\"\\nEstimación de películas totales: {len(keys) * 8500:,} películas\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error listando archivos: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJECUTAR PROCESAMIENTO PARALELO\n",
    "print(\"INICIANDO PROCESAMIENTO PARALELO CON LAMBDA\")\n",
    "\n",
    "# Procesar todos los archivos en paralelo\n",
    "total_duration = run_parallel_processing(keys, stats, max_workers=MAX_CONCURRENT_LAMBDAS, retry_attempts=RETRY_ATTEMPTS, delay_base=1)\n",
    "\n",
    "print(\"PROCESAMIENTO PARALELO COMPLETADO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTADÍSTICAS FINALES\n",
    "print(f\"TIEMPO TOTAL: {int(total_duration // 60)}m {int(total_duration % 60)}s\")\n",
    "print(f\"ARCHIVOS PROCESADOS: {stats.completed_files}\")\n",
    "print(f\"ÉXITOS: {len(stats.successful_files)}\")\n",
    "print(f\"FALLOS: {len(stats.failed_files)}\")\n",
    "\n",
    "if len(stats.failed_files) > 0:\n",
    "    print(f\"\\nARCHIVOS CON ERRORES:\")\n",
    "    for failed_info in stats.failed_files:\n",
    "        file_key = failed_info['file']\n",
    "        error = str(failed_info['error'])\n",
    "        print(f\"{file_key}: {error[:100]}...\")\n",
    "\n",
    "# Calcular estadísticas de rendimiento\n",
    "files_per_minute = (stats.completed_files / total_duration) * 60 if total_duration > 0 else 0\n",
    "estimated_movies = len(stats.successful_files) * 8500  # Promedio estimado por archivo\n",
    "\n",
    "print(f\"\\nRENDIMIENTO:\")\n",
    "print(f\"Velocidad: {files_per_minute:.1f} archivos/minuto\")\n",
    "print(f\"Películas procesadas: {stats.total_movies_processed:,}\")\n",
    "print(f\"Películas insertadas: {stats.total_movies_inserted:,}\")\n",
    "print(f\"Películas actualizadas: {stats.total_movies_updated:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64210abd",
   "metadata": {},
   "source": [
    "# Reprocesamiento de Archivos Fallidos\n",
    "\n",
    "Procesamiento para los archivos que fallaron en la ejecución anterior.\n",
    "\n",
    "**Archivos identificados:**\n",
    "- 4 por problemas de DB (necesitan correcciones de campo)\n",
    "\n",
    "**Antes de ejecutar:** Aplicar las correcciones de base de datos ejecutando `fix-database-fields.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de archivos fallidos identificados del procesamiento anterior\n",
    "failed_files_list = [\n",
    "    'movies1130000.json',\n",
    "    'movies1140000.json', \n",
    "    'movies1160000.json',\n",
    "    'movies1430000.json',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nueva instancia de estadísticas para el reprocesamiento\n",
    "retry_stats = ProcessingStats()\n",
    "retry_stats.total_files = len(failed_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2aab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJECUTAR REPROCESAMIENTO PARALELO DE ARCHIVOS FALLIDOS\n",
    "print(\"INICIANDO REPROCESAMIENTO PARALELO DE ARCHIVOS FALLIDOS\")\n",
    "\n",
    "# Volvemos a usar la función invoke_lambda_for_file para el reprocesamiento.\n",
    "retry_total_duration = run_parallel_processing(failed_files_list, retry_stats, max_workers=MAX_CONCURRENT_LAMBDAS, retry_attempts=5, delay_base=3)\n",
    "\n",
    "print(\"REPROCESAMIENTO DE FALLIDOS COMPLETADO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ba89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTADÍSTICAS FINALES DEL REPROCESAMIENTO\n",
    "print(f\"TIEMPO TOTAL REPROCESAMIENTO: {int(retry_total_duration // 60)}m {int(retry_total_duration % 60)}s\")\n",
    "print(f\"ARCHIVOS REPROCESADOS: {retry_stats.completed_files}\")\n",
    "print(f\"ÉXITOS: {len(retry_stats.successful_files)}\")\n",
    "print(f\"FALLOS: {len(retry_stats.failed_files)}\")\n",
    "\n",
    "if len(retry_stats.failed_files) > 0:\n",
    "    print(f\"\\nARCHIVOS AÚN CON ERRORES:\")\n",
    "    for failed_info in retry_stats.failed_files:\n",
    "        file_key = failed_info['file']\n",
    "        error = str(failed_info['error'])\n",
    "        print(f\"{file_key}: {error[:100]}...\")\n",
    "\n",
    "# Calcular estadísticas de rendimiento del reprocesamiento\n",
    "retry_files_per_minute = (retry_stats.completed_files / retry_total_duration) * 60 if retry_total_duration > 0 else 0\n",
    "\n",
    "print(f\"\\nRENDIMIENTO REPROCESAMIENTO:\")\n",
    "print(f\"Velocidad: {retry_files_per_minute:.1f} archivos/minuto\")\n",
    "print(f\"Películas procesadas: {retry_stats.total_movies_processed:,}\")\n",
    "print(f\"Películas insertadas: {retry_stats.total_movies_inserted:,}\")\n",
    "print(f\"Películas actualizadas: {retry_stats.total_movies_updated:,}\")\n",
    "\n",
    "retry_success_rate = (len(retry_stats.successful_files) / retry_stats.completed_files * 100) if retry_stats.completed_files > 0 else 0\n",
    "print(f\"Tasa de éxito: {retry_success_rate:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackaboss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
